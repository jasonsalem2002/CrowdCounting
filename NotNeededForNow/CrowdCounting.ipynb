{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhayEvSvS_fD",
        "outputId": "52d016a4-d79e-46b4-812b-462cd440b588"
      },
      "outputs": [],
      "source": [
        "!wget -O archive.zip \"https://storage.googleapis.com/kaggle-data-sets/233357/497494/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240404%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240404T133148Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0d43078cc6e9a67002b33f852d193916d3d36677e0c8f9617531feb6977b8db332cdce20b393a3adcd225b75a073cd0d81f7b61c2154c1b3e4b54e6f787579da71081d2de88a4c5351043f26b8e7590255fc38fc2b98974908193cbdbade90d824b5180743c9c30f4f61f3d7d306b8ce71a3dd0fa354e512a1206c8eab51b08309be574e29c1ccfc8969fc087b93434baeec000e1b1c18b2818b456e1e81aa229c89e23fcd3e5f7b16af67f4a7bee0353253c8bc0ed6cf79f433bbf719b1d0acb34fae712cc24a91030401bc9ed0afa92d8c5b1f240d556bc9e41ac000cfcc04a7d591c57bc6b1a073f40d2299b989bda59b7eaacb87c9ffc7c9873f0a279792\"\n",
        "\n",
        "# Unzip the downloaded file\n",
        "!unzip -q archive.zip\n",
        "\n",
        "# Clean up the downloaded zip file\n",
        "!rm archive.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdkAdOwbmn0n"
      },
      "source": [
        "# do not run this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5pGhCA5UMmL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torchvision.transforms import functional as F\n",
        "import torchvision\n",
        "\n",
        "def main():\n",
        "    # Instantiate the dataset\n",
        "    dataset = ShanghaiTechDataset(root='/content/ineed/ShanghaiTech', part='part_B', phase='train')\n",
        "\n",
        "    # Load an example\n",
        "    image, density = dataset[0]  # Just as an example, loading the first item\n",
        "\n",
        "    image = image.numpy().transpose(1, 2, 0)\n",
        "    density = density.numpy().squeeze()\n",
        "\n",
        "    # Displaying the image\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title('Original Image')\n",
        "\n",
        "    # Displaying the corresponding density map\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(density, cmap='jet')\n",
        "    plt.title('Density Map')\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYe4PBQJmr45"
      },
      "source": [
        "here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LeCLnGasW9yG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torchvision.transforms import functional as F\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBwQu6ZIVrgG",
        "outputId": "f25651e0-8b45-454f-e9f8-2c99e2372b4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jason\\Documents\\CrowdCounting\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jason\\Documents\\CrowdCounting\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training on device cuda...\n",
            "Epoch: 1, Batch: 100, Loss: 5.952202286607644e-07\n",
            "Epoch: 1, Batch: 200, Loss: 1.4289496448327554e-06\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from torchvision import models\n",
        "import os\n",
        "import glob\n",
        "import h5py\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import torchvision\n",
        "\n",
        "class ShanghaiTechDataset(Dataset):\n",
        "    def __init__(self, root, part='part_A', phase='train', transform=None):\n",
        "        self.root = os.path.join(root, part, f\"{phase}_data\")\n",
        "        self.image_paths = glob.glob(os.path.join(self.root, 'images', '*.jpg'))\n",
        "        self.gt_paths = [p.replace('.jpg', '.h5').replace('images', 'ground-truth') for p in self.image_paths]\n",
        "        self.transform = transform\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.image_paths[index]).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        gt_file = h5py.File(self.gt_paths[index], 'r')\n",
        "        target = np.asarray(gt_file['density'])\n",
        "\n",
        "        target = torch.from_numpy(target).float()\n",
        "\n",
        "        if len(target.shape) == 2:\n",
        "            target = target.unsqueeze(0)\n",
        "\n",
        "        target = F.resize(target, size=(96, 128), interpolation=transforms.InterpolationMode.BILINEAR)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "class SimplifiedCSRNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimplifiedCSRNet, self).__init__()\n",
        "        self.frontend_features = models.vgg16(pretrained=True).features[:23]\n",
        "        self.backend = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 1, kernel_size=1),\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.frontend_features(x)\n",
        "        x = self.backend(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "def train_model(model, train_loader, device, epochs=5):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-7)  # Decrease learning rate\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Learning rate scheduling\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    print(f\"Starting training on device {device}...\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f\"Epoch: {epoch+1}, Batch: {batch_idx+1}, Loss: {loss.item()}\")\n",
        "\n",
        "        scheduler.step()  # Step the scheduler\n",
        "\n",
        "        epoch_duration = time.time() - start_time\n",
        "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "        print(f'Epoch: {epoch+1}/{epochs}, Avg Loss: {avg_epoch_loss:.4f}, Duration: {epoch_duration:.2f} sec')\n",
        "\n",
        "    print(\"Training complete. Saving model...\")\n",
        "    torch.save(model.state_dict(), 'model_final.pth')\n",
        "    print(\"Model saved as model_final.pth\")\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    train_dataset = ShanghaiTechDataset(root='ShanghaiTech', part='part_B', phase='train', transform=None)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)  # Increase batch size\n",
        "    model = SimplifiedCSRNet()\n",
        "    train_model(model, train_loader, device)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "_PD6Dbhiq_xn",
        "outputId": "cd6c411b-ea12-41b0-c615-f7147c99d33d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class SimplifiedCSRNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimplifiedCSRNet, self).__init__()\n",
        "        self.frontend_features = models.vgg16(pretrained=True).features[:23]\n",
        "        self.backend = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 1, kernel_size=1),\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.frontend_features(x)\n",
        "        x = self.backend(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "# Function to load the model\n",
        "def load_model(model_path):\n",
        "    model = SimplifiedCSRNet()\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    return model\n",
        "\n",
        "# Function to predict the density map for a new image\n",
        "def predict_density_map(model, image_path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((768, 1024)),  # Resize if necessary to match training\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "    density_map = output.squeeze(0).squeeze(0).numpy()  # Remove batch and channel dimensions\n",
        "\n",
        "    return image, density_map\n",
        "\n",
        "# Function to visualize the original image and its density map\n",
        "def visualize_density_map(image, density_map):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title('Original Image')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(density_map, cmap='jet')\n",
        "    plt.title('Density Map')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Print the estimated total count\n",
        "    count = density_map.sum()\n",
        "    print(f'Estimated count: {count}')\n",
        "\n",
        "# Load the model\n",
        "model = load_model('model_final.pth')\n",
        "\n",
        "# Predict the density map for a new image\n",
        "image_path = 'test1.jpg'  # Update this path\n",
        "image, density_map = predict_density_map(model, image_path)\n",
        "\n",
        "# Visualize the original image and its density map\n",
        "visualize_density_map(image, density_map)\n",
        "# Before plotting the density map\n",
        "print(\"Density map raw values:\", density_map.flatten()[:10])  # Print first 10 values\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    density_map = output.squeeze(0).squeeze(0).numpy()  # Remove batch and channel dimensions\n",
        "\n",
        "# Calculate the total count\n",
        "count = density_map.sum()\n",
        "print(f'Estimated count: {count}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSHuOhmHrjgX",
        "outputId": "ddff9219-aa27-4904-d5f3-424ba3d0ec5a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class SimplifiedCSRNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimplifiedCSRNet, self).__init__()\n",
        "        self.frontend_features = models.vgg16(pretrained=True).features[:23]\n",
        "        self.backend = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 1, kernel_size=1),\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.frontend_features(x)\n",
        "        x = self.backend(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "# Load the model\n",
        "model = SimplifiedCSRNet()\n",
        "model.load_state_dict(torch.load('model_final.pth'))\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Prepare your image (assuming an image path 'new_image.jpg')\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((768, 1024)),  # Resize if necessary to match training\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "image = Image.open('/content/ineed/ShanghaiTech/part_B/train_data/images/IMG_102.jpg').convert('RGB')\n",
        "image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Predict the density map\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    density_map = output.squeeze(0).squeeze(0).numpy()  # Remove batch and channel dimensions\n",
        "\n",
        "# Calculate the total count\n",
        "count = density_map.sum()\n",
        "print(f'Estimated count: {count}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
